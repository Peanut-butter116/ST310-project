# Bundle into a workflow and fit
baseline_wf <- workflow() %>%
add_recipe(hmeq_recipe) %>%
add_model(log_spec)
baseline_fit <- fit(baseline_wf, data = train_data) # fit model to cleaned data
# Extract and tidy the fitted model
model_fit      <- pull_workflow_fit(baseline_fit)# extract the parsnip object
# Get class and probability predictions, plus the true outcome
preds <- baseline_fit %>%
predict(new_data = test_data, type = "prob") %>%      # .pred_no & .pred_yes
bind_cols(predict(baseline_fit, new_data = test_data, type = "class")) %>%  # .pred_class
bind_cols(test_data %>% select(BAD))                  # true BAD
# Compute four summary metrics on these predictions
baseline_tbl <- preds %>%
summarise(
Accuracy    = accuracy_vec(truth = BAD, estimate = .pred_class),   # overall correct rate
AUC         = roc_auc_vec(truth = BAD, .pred_1),                # area under ROC
Sensitivity = sens_vec(truth = BAD, estimate = .pred_class),      # true‐positive rate
Specificity = spec_vec(truth = BAD, estimate = .pred_class)       # true‐negative rate
)
# Print as a table
baseline_tbl %>%
kable(digits = 3, format = "html") %>%
kable_styling(full_width = FALSE)
################################################################################
hmeq_new <- read_csv("hmeq.csv") %>%
drop_na()            # drop missing values for ease
# split dataset into training & testing subsets
set.seed(456)
split <- initial_split(hmeq_new, prop = 0.8, strata = BAD)
train <- training(split)
test <- testing(split)
# prepare training matrix & target
y_train <- as.numeric(train$BAD) # keep target variable numeric
var_train <- train %>% select(where(is.numeric), -BAD)
scaled_train <- scale(var_train) # learn means & sds
X_train <- cbind(1, as.matrix(scaled_train)) # intercept + features
n_train <- nrow(X_train)
# define sigmoid function
sigmoid <- function(z) 1 / (1 + exp(-z))
# Initialize hyperparameters
theta <- rep(0, ncol(X_train))
alpha <- 0.001    # learning rate
iters <- 100      # num of iterations
# gradient descent process
for (i in seq_len(iters)) {
p_train <- sigmoid(X_train %*% theta)
grad <- t(X_train) %*% (p_train - y_train) / n_train
theta <- theta - alpha * grad
}
# prepare test matrix (same as above)
y_test <- as.numeric(test$BAD)
var_test <- test %>% select(where(is.numeric), -BAD)
scaled_test <- scale(
var_test,
center = attr(scaled_train, "scaled:center"),     # use train means
scale  = attr(scaled_train, "scaled:scale")       # use train sds
)
X_test <- cbind(1, as.matrix(scaled_test))
# predict on TEST set
p_pred_test    <- sigmoid(X_test %*% theta)
class_pred_test <- ifelse(p_pred_test > 0.5, 1, 0)
# build results tibble from TEST
results <- tibble(
truth       = factor(y_test, levels = c(0,1), labels = c("no","yes")),
.pred_yes   = p_pred_test,
.pred_class = factor(ifelse(p_pred_test > 0.5, "yes", "no"),
levels = c("no","yes"))
)
# true positive, true negative, false positive, false negative
tp <- sum(results$.pred_class == "yes" & results$truth == "yes")
tn <- sum(results$.pred_class == "no"  & results$truth == "no")
fp <- sum(results$.pred_class == "yes" & results$truth == "no")
fn <- sum(results$.pred_class == "no"  & results$truth == "yes")
# sensitivity & specificity
sensitivity <- tp / (tp + fn)
specificity <- tn / (tn + fp)
# accuracy & AUC
accuracy_cal <- mean(results$.pred_class == results$truth)
roc_obj <- roc(
response  = results$truth,
predictor = results$.pred_yes,
levels    = c("no", "yes"),   # first = control, second = case
direction = "<"               # ensures higher scores → more likely “yes”
)
auc_val <- as.numeric(auc(roc_obj))
# build a one‐row summary table
metrics_tbl <- tibble(
Accuracy    = accuracy_cal,
AUC         = auc_val,
Sensitivity = sensitivity,
Specificity = specificity
)
# print it
metrics_tbl %>%
kable(digits = 3, format = "html") %>%
kable_styling(full_width = FALSE)
################################################################################
# tree model
tree_spec <- decision_tree(
mode = "classification",
tree_depth = tune() ) %>%
set_engine("rpart")
# create workflow
tree_wf <- workflow() %>%
add_recipe(hmeq_recipe) %>%
add_model(tree_spec)
# cross-validation
cv_folds <- vfold_cv(train_data, v = 5, strata = BAD)
# grid tunning
tree_grid <- grid_regular(
tree_depth(range = c(1, 10)),        # depth from 1 to 10
levels = 5
)
tree_res <- tune_grid(
tree_wf,
resamples = cv_folds,
grid      = tree_grid,
metrics   = metric_set(roc_auc, accuracy)
)
# select best parameters by ROC AUC
best_tree_params <- select_best(tree_res, metric = "roc_auc")
# final model
final_tree_wf <- finalize_workflow(tree_wf, best_tree_params)
tree_fit <- last_fit(final_tree_wf, data_split)
# performance metrics
kable(collect_metrics(tree_fit))
# visualize the final tree
final_tree <- tree_fit %>% extract_fit_engine()
rpart.plot(final_tree, main = "Final Decision Tree")
# plot important features
final_tree %>%
vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
scale_y_continuous()
################################################################################
# define a recipe that expands features
highdim_rec <- recipe(BAD ~ ., data = train_data) %>%
# impute missing values
step_impute_median(all_numeric_predictors()) %>%   # median imputation for numeric variables
step_impute_mode(all_nominal_predictors()) %>%     # mode imputation for factors
# create dummies for categorical variables
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
# add log-transform numeric vars
step_log(all_numeric_predictors(), offset = 1) %>% # avoiding log(0)
# create all interactions among numeric predictors
step_interact(terms = ~ all_numeric_predictors():all_numeric_predictors()) %>%
# zero‐variance & normalization
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# specify a Lasso regression model
lasso_spec <- logistic_reg(
mode    = "classification",
penalty = tune(),
mixture = 1
) %>%
set_engine("glmnet")
# create the workflow
lasso_wf <- workflow() %>%
add_recipe(highdim_rec) %>%
add_model(lasso_spec)
# grid tunning
penalty_grid <- grid_regular(
penalty(range = c(-5, -1)),
levels = 30)
lasso_res <- tune_grid(
lasso_wf,
resamples = cv_folds,
grid      = penalty_grid,
metrics   = metric_set(roc_auc, accuracy)
)
# select the best penalty by ROC AUC
best_pen <- select_best(lasso_res, metric = "roc_auc")
# final model
final_lasso <- finalize_workflow(lasso_wf, best_pen) %>% last_fit(data_split)
# performance metrics
kable(collect_metrics(final_lasso))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#setwd('...')
library(tidyverse)
library(tidymodels)
library(broom)
library(kableExtra)
library(yardstick)
library(pROC)
library(corrplot)
library(dplyr)
library(vip)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(glmnet)
theme_set(theme_minimal())
# clear workspace
rm(list = ls())
# load data
hmeq <- read_csv("hmeq.csv")
################################################################################
# summarise basic info of all variables
#head(hmeq)
#summary(hmeq)
#str(hmeq)
################################################################################
# Convert character to factor
hmeq <- hmeq %>%
mutate(
BAD    = as.factor(BAD),
REASON = as.factor(REASON),
JOB    = as.factor(JOB)
)
################################################################################
# distribution of target variable
hmeq %>%
count(BAD) %>%
ggplot(aes(x = BAD, y = n, fill = BAD)) +
geom_col() +
labs(title = "Distribution of target variable - BAD", x = "BAD", y = "Frequency") +
theme_minimal()
################################################################################
# numeric variables
numeric_vars <- hmeq %>% select(where(is.numeric)) %>% names()
hmeq %>%
select(all_of(numeric_vars)) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
ggplot(aes(x = value)) +
facet_wrap(~ variable, scales = "free", ncol = 3) +
geom_histogram(bins = 30, color = "white", fill = "gray") +
labs(title = "Distribution of numeric variables", x = NULL, y = "Frequency") +
theme_minimal()
################################################################################
# categorical variables
categorical_vars <- hmeq %>%
select(where(is.factor)) %>%
select(-BAD) %>%    # exclude target because it has been shown before
names()
hmeq %>%
select(all_of(categorical_vars)) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
count(variable, value) %>%
ggplot(aes(x = value, y = n, fill = variable)) +
facet_wrap(~ variable, scales = "free", ncol = 2) +
geom_col(show.legend = FALSE) +
labs(title = "Distribution of categorical variables", x = NULL, y = "Frequency") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
################################################################################
# missing values
missing_values <- hmeq %>%
summarise(across(everything(), ~ sum(is.na(.)))) %>%
pivot_longer(everything(), names_to = "variable", values_to = "NAs")
kable(missing_values)
################################################################################
# correlation between numeric predictors
num_df <- hmeq %>%
select(all_of(numeric_vars)) %>%
drop_na()
corr <- cor(num_df)
corrplot(corr, method = "color", type = "lower", tl.cex = 0.8)
################################################################################
# split dataset into training & testing subsets
set.seed(123)
data_split <- initial_split(hmeq, prop = 0.8, strata = BAD)
train_data <- training(data_split)
test_data  <- testing(data_split)
# build a data preprocessing recipe (also work for other models)
hmeq_recipe <- recipe(BAD ~ ., data = train_data) %>%
step_impute_median(all_numeric_predictors()) %>%   # median imputation for numeric variables
step_impute_mode(all_nominal_predictors()) %>%     # mode imputation for factors
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% # create dummies
step_normalize(all_numeric_predictors())
################################################################################
# Define a baseline logistic‐regression model
log_spec <- logistic_reg(mode = "classification") %>%  # logistic regression model
set_engine("glm")
# Bundle into a workflow and fit
baseline_wf <- workflow() %>%
add_recipe(hmeq_recipe) %>%
add_model(log_spec)
baseline_fit <- fit(baseline_wf, data = train_data) # fit model to cleaned data
# Extract and tidy the fitted model
model_fit      <- pull_workflow_fit(baseline_fit)# extract the parsnip object
# Get class and probability predictions, plus the true outcome
preds <- baseline_fit %>%
predict(new_data = test_data, type = "prob") %>%      # .pred_no & .pred_yes
bind_cols(predict(baseline_fit, new_data = test_data, type = "class")) %>%  # .pred_class
bind_cols(test_data %>% select(BAD))                  # true BAD
# Compute four summary metrics on these predictions
baseline_tbl <- preds %>%
summarise(
Accuracy    = accuracy_vec(truth = BAD, estimate = .pred_class),   # overall correct rate
AUC         = roc_auc_vec(truth = BAD, .pred_1),                # area under ROC
Sensitivity = sens_vec(truth = BAD, estimate = .pred_class),      # true‐positive rate
Specificity = spec_vec(truth = BAD, estimate = .pred_class)       # true‐negative rate
)
# Print as a table
baseline_tbl %>%
kable(digits = 3, format = "html") %>%
kable_styling(full_width = FALSE)
################################################################################
hmeq_new <- read_csv("hmeq.csv") %>%
drop_na()            # drop missing values for ease
# split dataset into training & testing subsets
set.seed(456)
split <- initial_split(hmeq_new, prop = 0.8, strata = BAD)
train <- training(split)
test <- testing(split)
# prepare training matrix & target
y_train <- as.numeric(train$BAD) # keep target variable numeric
var_train <- train %>% select(where(is.numeric), -BAD)
scaled_train <- scale(var_train) # learn means & sds
X_train <- cbind(1, as.matrix(scaled_train)) # intercept + features
n_train <- nrow(X_train)
# define sigmoid function
sigmoid <- function(z) 1 / (1 + exp(-z))
# Initialize hyperparameters
theta <- rep(0, ncol(X_train))
alpha <- 0.001    # learning rate
iters <- 100      # num of iterations
# gradient descent process
for (i in seq_len(iters)) {
p_train <- sigmoid(X_train %*% theta)
grad <- t(X_train) %*% (p_train - y_train) / n_train
theta <- theta - alpha * grad
}
# prepare test matrix (same as above)
y_test <- as.numeric(test$BAD)
var_test <- test %>% select(where(is.numeric), -BAD)
scaled_test <- scale(
var_test,
center = attr(scaled_train, "scaled:center"),     # use train means
scale  = attr(scaled_train, "scaled:scale")       # use train sds
)
X_test <- cbind(1, as.matrix(scaled_test))
# predict on TEST set
p_pred_test    <- sigmoid(X_test %*% theta)
class_pred_test <- ifelse(p_pred_test > 0.5, 1, 0)
# build results tibble from TEST
results <- tibble(
truth       = factor(y_test, levels = c(0,1), labels = c("no","yes")),
.pred_yes   = p_pred_test,
.pred_class = factor(ifelse(p_pred_test > 0.5, "yes", "no"),
levels = c("no","yes"))
)
# true positive, true negative, false positive, false negative
tp <- sum(results$.pred_class == "yes" & results$truth == "yes")
tn <- sum(results$.pred_class == "no"  & results$truth == "no")
fp <- sum(results$.pred_class == "yes" & results$truth == "no")
fn <- sum(results$.pred_class == "no"  & results$truth == "yes")
# sensitivity & specificity
sensitivity <- tp / (tp + fn)
specificity <- tn / (tn + fp)
# accuracy & AUC
accuracy_cal <- mean(results$.pred_class == results$truth)
roc_obj <- roc(
response  = results$truth,
predictor = results$.pred_yes,
levels    = c("no", "yes"),   # first = control, second = case
direction = "<"               # ensures higher scores → more likely “yes”
)
auc_val <- as.numeric(auc(roc_obj))
# build a one‐row summary table
metrics_tbl <- tibble(
Accuracy    = accuracy_cal,
AUC         = auc_val,
Sensitivity = sensitivity,
Specificity = specificity
)
# print it
metrics_tbl %>%
kable(digits = 3, format = "html") %>%
kable_styling(full_width = FALSE)
################################################################################
# tree model
tree_spec <- decision_tree(
mode = "classification",
tree_depth = tune() ) %>%
set_engine("rpart")
# create workflow
tree_wf <- workflow() %>%
add_recipe(hmeq_recipe) %>%
add_model(tree_spec)
# cross-validation
cv_folds <- vfold_cv(train_data, v = 5, strata = BAD)
# grid tunning
tree_grid <- grid_regular(
tree_depth(range = c(1, 10)),        # depth from 1 to 10
levels = 5
)
tree_res <- tune_grid(
tree_wf,
resamples = cv_folds,
grid      = tree_grid,
metrics   = metric_set(roc_auc, accuracy)
)
# select best parameters by ROC AUC
best_tree_params <- select_best(tree_res, metric = "roc_auc")
# final model
final_tree_wf <- finalize_workflow(tree_wf, best_tree_params)
tree_fit <- last_fit(final_tree_wf, data_split)
# performance metrics
kable(collect_metrics(tree_fit))
# visualize the final tree
final_tree <- tree_fit %>% extract_fit_engine()
rpart.plot(final_tree, main = "Final Decision Tree")
# plot important features
final_tree %>%
vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
scale_y_continuous()
################################################################################
# define a recipe that expands features
highdim_rec <- recipe(BAD ~ ., data = train_data) %>%
# impute missing values
step_impute_median(all_numeric_predictors()) %>%   # median imputation for numeric variables
step_impute_mode(all_nominal_predictors()) %>%     # mode imputation for factors
# create dummies for categorical variables
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
# add log-transform numeric vars
step_log(all_numeric_predictors(), offset = 1) %>% # avoiding log(0)
# create all interactions among numeric predictors
step_interact(terms = ~ all_numeric_predictors():all_numeric_predictors()) %>%
# zero‐variance & normalization
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
# specify a Lasso regression model
lasso_spec <- logistic_reg(
mode    = "classification",
penalty = tune(),
mixture = 1
) %>%
set_engine("glmnet")
# create the workflow
lasso_wf <- workflow() %>%
add_recipe(highdim_rec) %>%
add_model(lasso_spec)
# grid tunning
penalty_grid <- grid_regular(
penalty(range = c(-5, -1)),
levels = 30)
lasso_res <- tune_grid(
lasso_wf,
resamples = cv_folds,
grid      = penalty_grid,
metrics   = metric_set(roc_auc, accuracy)
)
# select the best penalty by ROC AUC
best_pen <- select_best(lasso_res, metric = "roc_auc")
# final model
final_lasso <- finalize_workflow(lasso_wf, best_pen) %>% last_fit(data_split)
# performance metrics
kable(collect_metrics(final_lasso))
# model specification
rf_spec <- rand_forest(
mode    = "classification",
trees   = 500,
mtry    = tune(),
min_n   = tune()
) %>%
set_engine("randomForest")
# create workflow
rf_wf <- workflow() %>%
add_recipe(hmeq_recipe) %>%
add_model(rf_spec)
# grid tunning
rf_grid <- grid_regular(
mtry(range = c(5, 18)),
min_n(range = c(1, 10)),
levels = 5
)
rf_res <- tune_grid(
rf_wf,
resamples = cv_folds,
grid      = rf_grid,
metrics   = metric_set(accuracy, roc_auc)
)
# select best by roc_auc
best_rf <- select_best(rf_res, metric = "roc_auc")
# final model
final_rf <- finalize_workflow(rf_wf, best_rf) %>% last_fit(data_split)
# performance metrics
kable(collect_metrics(final_rf))
#XGBoost
# model specification
xgb_spec <- boost_tree(
mode        = "classification",
trees       = 500,
tree_depth  = tune(),
learn_rate  = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost")
# create workflow
xgb_wf <- workflow() %>%
add_recipe(hmeq_recipe) %>%
add_model(xgb_spec)
# grid tunning
xgb_grid <- grid_regular(
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.001, 0.1)),
loss_reduction(range = c(0.001, 10)),
levels = 5
)
xgb_res <- tune_grid(
xgb_wf,
resamples = cv_folds,
grid = xgb_grid,
metrics = metric_set(accuracy, roc_auc)
)
# select best by roc_auc
best_xgb <- select_best(xgb_res, metric = "roc_auc")
# final model
final_xgb <- finalize_workflow(xgb_wf, best_xgb) %>% last_fit(data_split)
# performance metrics
kable(collect_metrics(final_xgb))
